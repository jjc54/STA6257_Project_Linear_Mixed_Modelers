[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Mixed Models (LMMs)",
    "section": "",
    "text": "Linear mixed models (LMMs) are statistical models that account for both fixed and random effects. Please follow along as we provide a systematic review of LMMs, their applications, their limitations, and more. Importantly, we will develop a report and by-example analysis of LMMs in R.\nHere’s a snapshot of the analysis process our team is currently going through:\n\nFeel free to meet our team on the About tab and review our individual literature contributions on the Literature tab."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "This is the directory for the UWF STA 6257 group: The Linear Mixed Modelers. Thank you for stopping by our page!\n\n\n\n\n\n\n\n\n\nJoshua J. Cook, M.S., ACRP-PM, CCRC\nJoshua J. Cook, M.S., ACRP-PM, CCRC is a recent graduate of Wake Forest University (WFU) where he earned his Master of Science for Clinical Research Management. He is a current graduate student at the University of West Florida (UWF) studying Data Science while working at the university as an Adjunct Professor. Joshua worked in the field of clinical research for nearly three years, starting in neurology clinical trials and then specializing in orthopedic regenerative medicine as a Research Quality Analyst. He has published his undergraduate honors thesis, entitled “Endurance exercise-mediated metabolic reshuffle attenuates high-caloric diet-induced non-alcoholic fatty liver disease” in the Annals of Hepatology and has recently submitted several orthopedic research papers to various journals. He has also presented his research at over ten unique conferences at the local, state, and national levels with topics spanning from the impact of blood sugar on Alzheimer’s Disease to publication metric tracking with R and Microsoft Power BI®. Joshua has developed a passion for bench-to-bedside research and aims to synthesize his knowledge of the biomedical sciences, clinical research, and data science to become a physician-scientist capable of integrating clinical care with clinical research in a way that maximizes evidence-based care options for his patients.\n\n\n\n\n\n\n\n\n\n\n\n\nSyed Ahzaz H. Shah, B.S.\nSyed Ahzaz Shah, a skilled Software Engineer, excels in developing scalable solutions using Python, SQL, Java, and more. Currently working as software engineer, he contributes to API design, debugging, and deployment, showcasing a commitment to engineering best practices. Syed holds a Bachelor’s in Computer Science, complemented by certifications in AWS Cloud Practitioner and Google Data Analytics.\n\n\n\n\n\n\n\n\n\n\n\n\nJacob Hernandez, B.S.\nJacob Hernadnez graduated from the California State University of Long Beach with a degree in Chemical Engineering. Currently, he is Data Analyst for an Aeronautics and Defense Company in California where he uses Python, SQL and Tableau to deliver high quality data products to important stake holders. He has recieved his Google Cloud Data Engineering Certifications and TensorFlow Developer Certification which has helped him produce and deploy machine learning projects for his personal portfolio.\n\n\n\n\n\n\n\n\n\n\n\n\nSara Basili, M.S.\nSara Basili obtained her Master of Science in Computer Science with a concentration in Artificial Intelligence from the University of New Orleans (UNO) in December 2023. She is currently pursuing her Master of Science in Data Science at the University of West Florida (UWF), while working as an AI researcher and directing operations at an Artificial Intelligence company based in New Orleans. Prior to her current roles, she gained over a year of insightful experience working as a Crime Analyst for the District Attorney’s office in New Orleans. Sara earned her B.S. in Statistics from Università degli Studi di Roma “La Sapienza” in March 2021, where she authored a thesis entitled “The Social Impact of Gallup Polls to Measure Public Opinion: An Empirical Analysis of the Knight Communities.” She is proficient in a variety of programming languages, including SQL, R, Python, SAS, and Java, which are central to her work in both artificial intelligence and data science."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "print(\"TBD\")\n\n[1] \"TBD\""
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "",
    "text": "Linear mixed-effects models (LMMs) are advanced statistical tools designed to analyze data that exhibit complex structures, such as hierarchical organization, repeated measures, and random effects. These models are particularly useful when data violate the assumptions of traditional ANOVA or regression methods, such as the independence of observations, homoscedasticity, and normality of residuals. LMMs accommodate intra-subject differences, allowing for both fixed effects, which are consistent across individuals, and random effects, which vary among subjects or groups.\nThe implementation of LMMs has been facilitated by various software packages and programming languages. Brown (Brown 2021) provides a comprehensive guide to implementing LMMs in R, a widely used statistical programming language, offering a step-by-step walkthrough of model syntax without delving deeply into complex mathematical foundations. Additionally, the lme4 package, as detailed by Bates et al. (Bates et al. 2015), represents a significant evolution in computational methods for fitting mixed models, offering efficient tools and simplified modeling processes for R users, especially for models with crossed random effects. Pymer4, developed by Jolly (Jolly 2018), bridges R and Python, offering Python users a flexible and integrated tool for linear mixed modeling by leveraging the capabilities of R’s lme4 package. This tool enhances the analytical capabilities within the Python ecosystem, making advanced statistical methods more accessible to a broader audience.\nLMMs find applications across various scientific domains, each with its unique data structures and analytical challenges. The paper by Lee and Shang (Lee and Shang n.d.) explores the impact of missing data on the estimation and selection in LMMs, highlighting the challenges and proposing a method to record missingness using an indicator-based matrix. This approach is critical for ensuring model accuracy in the presence of missing data, a common issue in real-world datasets. Wang et al. (Wang et al. 2022) illustrate the application of LMMs in cardiothoracic surgery outcomes research, using a case study of homograft pulmonary valve replacement data to demonstrate the model’s ability to handle repeated measurements and provide more nuanced understandings of clinical outcomes. Aarts et al [Aarts et al. (2015)] demonstrates mulilevel design experiements in neurscciene and how using linear models on multilevel data can result in increase in false positives. Magezi (Magezi 2015) highlights the use of LMMs in within-participant psychology experiments, addressing the complexities of repeated measures and nested data structures common in psychological research. Harrison et al. (Harrison et al. 2018) and Bolker et al. (Bolker et al. 2009) discuss the application of LMMs and generalized linear mixed models (GLMMs) in ecology, emphasizing their utility in analyzing ecological data that involve complex relationships and hierarchical data structures with GRU. Grueber et al (GRUEBER et al. 2011) another ecology research, paper focuses on the model averaging and information theoritac with LMMS as an alternative to traditional null hypothesis testing. In the medical field, LMMs are employed to model pandemic-induced mortality changes, as demonstrated by Verbeeck et al. (Verbeeck et al. 2023), and to analyze longitudinal health-related quality of life data in cancer clinical trials, as discussed by Touraine et al. (Touraine et al. 2023).\nThe paper “To transform or not to transform: using generalized linear mixed models to analyse reaction time data” by Lo and Andrews (Lo and Andrews 2015) challenges the common practice of transforming reaction time data in cognitive psychology, advocating for GLMMs as a more robust alternative. The “LEVEL” guidelines proposed by Monsalves et al. (Monsalves et al. 2020) aim to standardize the reporting of multilevel data and analyses, enhancing comparability across studies. Piepho’s study (Piepho 1999) on analyzing disease incidence data with GLMMs underscores the inadequacy of traditional methods like ANOVA for such data, highlighting GLMMs’ flexibility. The simulation study by Pusponegoro et al. (Pusponegoro et al. 2017) on children’s growth differences emphasizes the importance of choosing the appropriate covariance structure in LMMs for longitudinal data. Lastly, the framework introduced by Steibel et al. (Steibel et al. 2009) for analyzing RT-PCR data with LMMs showcases the method’s statistical power and flexibility, offering a significant advancement over traditional analysis methods. LMMs are used in a wide array of disciplines, but also in varying study designs, as shown in Table 1.\nTable 1. Systematic Review of LMM Use-cases (Casals et al. 2014)\n\nThe strengths of LMMs lie in their flexibility to model complex data structures and their ability to handle missing data, making them a powerful tool for a wide range of scientific inquiries. However, their application is not without challenges. Peng and Lu (Peng and Lu 2012) address the difficulty of variable selection and parameter estimation in LMMs, proposing an iterative procedure to improve model accuracy. Barr (Barr 2013) critiques existing guidelines for testing interactions within LMMs, proposing new guidelines to ensure more reliable results. The work by Tu (Tu 2015) on GLMMs for network meta-analyses showcases how mixed models have evolved to tackle complex data, enhancing the accuracy of combining different studies. On the other hand, Fokkema et al. (Marjolein Fokkema and Wolpert 2021) introduce GLMM trees, merging machine learning with mixed models to improve predictions and analysis, particularly useful in mental health research. Despite their robustness, as noted by Schielzeth et al. (Schielzeth et al. 2020), LMMs require careful evaluation of model assumptions and may present computational challenges, especially with high-dimensional datasets.\nThe literature reviewed here collectively emphasizes the versatility, robustness, and broad applicability of LMMs and GLMMs across various fields of research. Despite their advantages, the importance of careful model selection, acknowledgment of limitations, and the potential need for more complex models such as joint models in certain scenarios are also highlighted. As the use of LMMs continues to grow, the development of standardized processes, such as the LEVEL framework (Monsalves et al. 2020) and the 10 protocol put forth by (Zuur and Ieno 2016), and user-friendly tools will be crucial in ensuring the accurate and effective application of these models in research."
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe paper presents the lme4 package for R, which facilitates the fitting of linear mixed-effects models. The authors aim to articulate the package’s capabilities in evaluating the profiled deviance or REML criterion for linear mixed models and to explain the representation and optimization of such models for parameter estimation.\nWhy is it important?\nThe significance of this paper lies in its contribution to the field of computational methods for fitting mixed models—an area with many open problems. The lme4 package represents an evolution in this domain, offering more efficient computational tools and a syntax that simplifies the modeling process, especially for models with crossed random effects.\nHow is it solved? – methods\nThe package utilizes maximum likelihood or restricted maximum likelihood (REML) estimates for linear mixed-effects model parameters, employing numerical representation and optimization functions within R. The paper delves into the model’s structure, the evaluative steps for the profiled deviance or REML criterion, and the class structure representing such models, highlighting the improvements over previous formulations.\nResults/limitations, if any.\nThe document focuses more on methodology than specific results or limitations. It details the improvement over the nlme package, addressing efficient linear algebra tools and the incorporation of profile likelihood confidence intervals on random-effects parameters. The paper emphasizes the ongoing development of the lme4 package, acknowledging the need for stability and usability for a broad range of applications.\n\n\n\nWhat is the goal of the paper?\nThe goal of the paper is to provide a detailed introduction to developing and interpreting linear mixed-effects models for repeated measurements in the context of cardiothoracic surgery outcomes research. The paper uses a dataset on patients undergoing surgical pulmonary valve replacement to illustrate the steps of developing such models for clinician researchers.\nWhy is it important?\nThis work is important because the emergence of large cardio-thoracic surgery datasets, including repeated measurements over time, presents an opportunity to apply advanced modeling of outcomes. Linear mixed-effects models offer a more nuanced understanding of these outcomes compared to traditional methods, which is crucial for enhancing clinical decision-making and patient care.\nHow is it solved? – methods\nThe authors used a retrospective dataset containing serial echocardiographic measurements from patients who underwent surgical pulmonary valve replacement at Erasmus MC between 1986 and 2017. The paper discusses the construction of the model, including dealing with missing values, correlated variables, and multicollinearity. It also covers model specification, variable selection, addressing nonlinearity, and interpretation of results. An R script is provided for implementing the model.\nResults/limitations, if any.\nThe paper illustrates the construction of the model, including essential aspects such as theories of linear mixed-effects models, missing values, collinearity, interaction, nonlinearity, model specification, and results interpretation. It shows that linear mixed-effects models provide a more detailed view of repeated measurements and give more valid estimates compared to linear regression models, especially in the context of cardio-thoracic surgery outcomes research. Limitations related to model assumptions, such as linearity and normal distribution of residuals, are addressed through transformations and statistical tests.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to provide a comprehensive guide on the use and application of Generalized Linear Mixed Models (GLMMs) for ecologists and evolutionary biologists dealing with nonnormal data types, such as counts or proportions, which often do not fit well with classical statistical procedures. The paper aims to clarify the use of GLMMs, given the popularity of these models in recent years.\nWhy is it important?\nThe importance of this paper lies in its attempt to introduce GLMMs for biologists, where data often fall outside the scope of methods taught in introductory statistics classes. The paper highlights the limitations of traditional shortcuts like data transformation or ignoring random effects and advocates for GLMMs as a more appropriate statistical approach for nonnormal data with random effects.\nHow is it solved? – methods\nThe paper reviews the use and misuse of GLMMs in biology, discusses estimation and inference, and summarizes best-practice data analysis procedures. It emphasizes the need for researchers to match their statistical approaches to their data, rather than forcing data into classical statistical frameworks. The paper discusses various estimation algorithms for fitting GLMMs, including maximum likelihood (ML), pseudo- and penalized quasilikelihood (PQL), Laplace approximations, Gauss-Hermite quadrature (GHQ), and Markov chain Monte Carlo (MCMC) algorithms.\nResults/limitations, if any.\nWhile the paper provides a broad overview of GLMM procedures and best practices, it also acknowledges the challenges and controversies in statistical issues such as null hypothesis testing, stepwise regression, and the use of Bayesian statistics. It highlights that GLMMs are powerful tools but can be challenging to use, even for statisticians, due to computational difficulties in estimating parameters, especially for complex models or large numbers of random effects.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to introduce linear mixed-effects models (LMMs) as a versatile tool for analyzing data from within-participant psychology experiments. It seeks to address the limitations of traditional analysis methods like ANOVA in handling complex data structures, such as those involving repeated measures or nested designs. The paper also introduces LMMgui, a free, graphical user interface designed to facilitate the use of LMMs for researchers using R.\nWhy is it important?\nThe importance of this work lies in its potential to enhance the analysis of experimental psychology data by providing a more flexible and robust statistical tool that can handle the complexities of within-participant designs, such as pseudoreplication and missing data. By offering a user-friendly interface for LMM analysis, the paper aims to make advanced statistical methods more accessible to researchers, thereby improving the quality and interpretability of psychological research.\nHow is it solved? – methods\nThe paper discusses the theoretical foundation of LMMs, explaining how they can accommodate various data structures and assumptions that are commonly encountered in psychology experiments. It contrasts LMMs with traditional repeated-measures ANOVA, highlighting the advantages of LMMs in terms of their flexibility and fewer stringent assumptions. The introduction of LMMgui is a significant methodological contribution, providing a step-by-step guide on how to use this tool to specify and compare different LMMs for data analysis.\nResults/limitations, if any.\nWhile the paper primarily serves as a tutorial and does not present results from a specific study, it effectively demonstrates the application of LMMs through hypothetical examples. These examples illustrate how LMMs can be used to analyze data from within-participant designs, accounting for random effects and complex variance-covariance structures. The paper acknowledges the challenges in interpreting LMM results and the potential for increased Type I error rates in certain conditions, emphasizing the need for careful model comparison and validation.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to estimate baseline mortality (mortality under non-pandemic conditions for Belgium and the Netherlands using a linear mixed model (LMM), which can account for both fixed and random effects. If baseline mortality can be modeled, then excess mortality (the measure of the increase in mortality from all causes during a specific time period) can be used to evaluate the impact of COVID-19 on mortality.\nWhy is it important?\nHistorically, 5-year weekly averages have been used to determine baseline mortality. However, this excludes year-specific trends in mortality and the effects of historical excess mortality (ex: past influenza breakouts or heat waves). Using a LMM is important because it allows for more accurate modeling that accounts for these factors in the form of random effects.\nHow is it solved? – methods\nThe paper proposes a general linear mixed model to model weekly mortality as Ytj, with t = 1,…,52 weeks and by year j = 2009…,2020.\nThe model is then adjusted to: model the cyclic pattern from year to year via random effects of Fourier terms, and reduce the influence of historical excess mortality (as mentioned above) by downweighing the residuals.\nResults/limitations, if any.\nSeveral statistics were used to evaluate the model’s forecasting accuracy, including the likelihood ratio test (LRT) and the root mean square error % (RMSE%). The models were fitted to historical mortality year from 2009- week 10 of 2020. The remaining 42 weeks of 2020 were forecasted using the LMM, along with the 5-year average model, and the ground truth data. The models all performed well, so an overall recommendation to include the down-weight procedure for past excess mortality and to include a serial correlation structure were made. The LMM did fit the mortality data better and two years were better predicted compared to the 5-year weekly average models. Many limitations exist, including differences in the reporting of COVID-19 deaths in Belgium and the Netherlands, and across the world. Additionally, it is unclear if the added complexity of LMMs provide a significant benefit over 5-year weekly average models in years besides 2014 and 2016.\n\n\n\nWhat is the goal of the paper?\nThe goal of this paper is to address a potential shortcoming of LMMs, which is when data is not missing at random, but instead, data is missing that is dependent on the health-related quality-of-life of the patient (i.e., data is missing because quality-of-life – the outcome - decreased). Viewing missing data like this, a survival model may be more appropriate. Or, as this paper suggests, a joint model (JM) that includes both LMM and survival sub-models.\nWhy is it important?\nThis concept is important because clinical trials are frequently employing LMMs to evaluate longitudinal data. Clinical trials involve human subjects, which are known to be more variable compared to benchtop studies. This includes variation in the completeness of patient reported outcome measures (PROMs), especially for longitudinal studies. Many times, this variation is random, and independent of the outcome measure. However, if the outcome measure itself is having an effect on the completeness, then it could represent a major bias limitation of LMMs that should be addressed using the proposed joint models.\nHow is it solved? – methods\nThis paper first introduces the LMM that is traditional in longitudinal clinical studies like this. Then, to account for the fact that observations in quality-of-life scores ends with a dropout event, a joint model is created by linking the LMM to a survival model with shared parameters.\nThis JM was then evaluated using historical clinical trial data where a standard LMM was utilized, and in several simulation studies where extreme examples of this dependence was introduced.\nResults/limitations, if any.\nThis paper showed that poor quality-of-life scores are associated with drop out. Therefore, LMMs should be avoided when analyzing this data in clinical trials. The LMMs in both the historical studies and in the simulations were overly optimistic in estimating the quality-of-life scores. Specifically, the LMM overestimates the slope governing the prediction trajectory in both treatment and control arms. The LMM is also more optimistic for the control arm than the experimental arm, despite the protective effect of treatment on the quality-of-life score."
  },
  {
    "objectID": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r---violet-a.-brown-2021-uwf.edu",
    "href": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r---violet-a.-brown-2021-uwf.edu",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe article aims to introduce the utility of linear mixed-effects models (LMMs) over traditional ANOVA or regression methods, emphasizing their applicability in handling intra-subject differences and their flexibility with missing values and outliers. This was specifically done in R.\nWhy is it important?\nLMMs are highlighted for their ability to not assume independence of observations, making them particularly suitable for psychological research where within-subject differences are common.\nHow is it solved? – methods\nThe article employs a psychology-based dataset to demonstrate the implementation of LMMs using R. It focuses on explaining the concepts of fixed effects and random effects in LMMs, providing the data and code for readers to follow along without delving deeply into the mathematical foundations of LMMs.\nResults/limitations, if any.\nThe primary contribution is the detailed walkthrough of LME model syntax in R and interpretation within the context of psychological data. The article suggests that the methods described should be generalizable across various industries, although it is based on a specific psychology dataset."
  },
  {
    "objectID": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology---pmc-nih.gov",
    "href": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology---pmc-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThis article aims to demonstrate the application of LMMs and multi-model inference within the field of ecology, focusing on error and model selection using biological data.\nWhy is it important?\nThe significance of the article lies in its broad overview of LMMs in the context of ecological data, addressing the challenges of error and model selection in ecological research.\nHow is it solved? – methods\nThe methodology includes a detailed examination of information theory and multi-model inference, applied to example ecological data. The article provides data and code for replication and further exploration.\nResults/limitations, if any.\nWhile the article offers valuable insights into the use of LMMs in ecology, including data and code for practical application, it primarily serves as an introductory piece, potentially leaving out more advanced aspects of LMMs and multi-model inference."
  },
  {
    "objectID": "literature.html#generalized-linear-mixed-models-a-practical-guide-for-ecology-and-evolution---sciencedirect",
    "href": "literature.html#generalized-linear-mixed-models-a-practical-guide-for-ecology-and-evolution---sciencedirect",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to provide a comprehensive guide on the use and application of Generalized Linear Mixed Models (GLMMs) for ecologists and evolutionary biologists dealing with nonnormal data types, such as counts or proportions, which often do not fit well with classical statistical procedures. The paper aims to clarify the use of GLMMs, given the popularity of these models in recent years.\nWhy is it important?\nThe importance of this paper lies in its attempt to introduce GLMMs for biologists, where data often fall outside the scope of methods taught in introductory statistics classes. The paper highlights the limitations of traditional shortcuts like data transformation or ignoring random effects and advocates for GLMMs as a more appropriate statistical approach for nonnormal data with random effects.\nHow is it solved? – methods\nThe paper reviews the use and misuse of GLMMs in biology, discusses estimation and inference, and summarizes best-practice data analysis procedures. It emphasizes the need for researchers to match their statistical approaches to their data, rather than forcing data into classical statistical frameworks. The paper discusses various estimation algorithms for fitting GLMMs, including maximum likelihood (ML), pseudo- and penalized quasilikelihood (PQL), Laplace approximations, Gauss-Hermite quadrature (GHQ), and Markov chain Monte Carlo (MCMC) algorithms.\nResults/limitations, if any.\nWhile the paper provides a broad overview of GLMM procedures and best practices, it also acknowledges the challenges and controversies in statistical issues such as null hypothesis testing, stepwise regression, and the use of Bayesian statistics. It highlights that GLMMs are powerful tools but can be challenging to use, even for statisticians, due to computational difficulties in estimating parameters, especially for complex models or large numbers of random effects."
  },
  {
    "objectID": "literature.html#frontiers-linear-mixed-effects-models-for-within-participant-psychology-experiments-an-introductory-tutorial-and-free-graphical-user-interface-lmmgui-frontiersin.org",
    "href": "literature.html#frontiers-linear-mixed-effects-models-for-within-participant-psychology-experiments-an-introductory-tutorial-and-free-graphical-user-interface-lmmgui-frontiersin.org",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to introduce linear mixed-effects models (LMMs) as a versatile tool for analyzing data from within-participant psychology experiments. It seeks to address the limitations of traditional analysis methods like ANOVA in handling complex data structures, such as those involving repeated measures or nested designs. The paper also introduces LMMgui, a free, graphical user interface designed to facilitate the use of LMMs for researchers using R.\nWhy is it important?\nThe importance of this work lies in its potential to enhance the analysis of experimental psychology data by providing a more flexible and robust statistical tool that can handle the complexities of within-participant designs, such as pseudoreplication and missing data. By offering a user-friendly interface for LMM analysis, the paper aims to make advanced statistical methods more accessible to researchers, thereby improving the quality and interpretability of psychological research.\nHow is it solved? – methods\nThe paper discusses the theoretical foundation of LMMs, explaining how they can accommodate various data structures and assumptions that are commonly encountered in psychology experiments. It contrasts LMMs with traditional repeated-measures ANOVA, highlighting the advantages of LMMs in terms of their flexibility and fewer stringent assumptions. The introduction of LMMgui is a significant methodological contribution, providing a step-by-step guide on how to use this tool to specify and compare different LMMs for data analysis.\nResults/limitations, if any.\nWhile the paper primarily serves as a tutorial and does not present results from a specific study, it effectively demonstrates the application of LMMs through hypothetical examples. These examples illustrate how LMMs can be used to analyze data from within-participant designs, accounting for random effects and complex variance-covariance structures. The paper acknowledges the challenges in interpreting LMM results and the potential for increased Type I error rates in certain conditions, emphasizing the need for careful model comparison and validation."
  },
  {
    "objectID": "literature.html#a-linear-mixed-model-to-estimate-covid-19-induced-excess-mortality---pubmed-nih.gov",
    "href": "literature.html#a-linear-mixed-model-to-estimate-covid-19-induced-excess-mortality---pubmed-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to estimate baseline mortality (mortality under non-pandemic conditions for Belgium and the Netherlands using a linear mixed model (LMM), which can account for both fixed and random effects. If baseline mortality can be modeled, then excess mortality (the measure of the increase in mortality from all causes during a specific time period) can be used to evaluate the impact of COVID-19 on mortality.\nWhy is it important?\nHistorically, 5-year weekly averages have been used to determine baseline mortality. However, this excludes year-specific trends in mortality and the effects of historical excess mortality (ex: past influenza breakouts or heat waves). Using a LMM is important because it allows for more accurate modeling that accounts for these factors in the form of random effects.\nHow is it solved? – methods\nThe paper proposes a general linear mixed model to model weekly mortality as Ytj, with t = 1,…,52 weeks and by year j = 2009…,2020.\nThe model is then adjusted to: model the cyclic pattern from year to year via random effects of Fourier terms, and reduce the influence of historical excess mortality (as mentioned above) by downweighing the residuals.\nResults/limitations, if any.\nSeveral statistics were used to evaluate the model’s forecasting accuracy, including the likelihood ratio test (LRT) and the root mean square error % (RMSE%). The models were fitted to historical mortality year from 2009- week 10 of 2020. The remaining 42 weeks of 2020 were forecasted using the LMM, along with the 5-year average model, and the ground truth data. The models all performed well, so an overall recommendation to include the down-weight procedure for past excess mortality and to include a serial correlation structure were made. The LMM did fit the mortality data better and two years were better predicted compared to the 5-year weekly average models. Many limitations exist, including differences in the reporting of COVID-19 deaths in Belgium and the Netherlands, and across the world. Additionally, it is unclear if the added complexity of LMMs provide a significant benefit over 5-year weekly average models in years besides 2014 and 2016."
  },
  {
    "objectID": "literature.html#when-a-joint-model-should-be-preferred-over-a-linear-mixed-model-for-analysis-of-longitudinal-health-related-quality-of-life-data-in-cancer-clinical-trials---pubmed-nih.gov",
    "href": "literature.html#when-a-joint-model-should-be-preferred-over-a-linear-mixed-model-for-analysis-of-longitudinal-health-related-quality-of-life-data-in-cancer-clinical-trials---pubmed-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of this paper is to address a potential shortcoming of LMMs, which is when data is not missing at random, but instead, data is missing that is dependent on the health-related quality-of-life of the patient (i.e., data is missing because quality-of-life – the outcome - decreased). Viewing missing data like this, a survival model may be more appropriate. Or, as this paper suggests, a joint model (JM) that includes both LMM and survival sub-models.\nWhy is it important?\nThis concept is important because clinical trials are frequently employing LMMs to evaluate longitudinal data. Clinical trials involve human subjects, which are known to be more variable compared to benchtop studies. This includes variation in the completeness of patient reported outcome measures (PROMs), especially for longitudinal studies. Many times, this variation is random, and independent of the outcome measure. However, if the outcome measure itself is having an effect on the completeness, then it could represent a major bias limitation of LMMs that should be addressed using the proposed joint models.\nHow is it solved? – methods\nThis paper first introduces the LMM that is traditional in longitudinal clinical studies like this. Then, to account for the fact that observations in quality-of-life scores ends with a dropout event, a joint model is created by linking the LMM to a survival model with shared parameters.\nThis JM was then evaluated using historical clinical trial data where a standard LMM was utilized, and in several simulation studies where extreme examples of this dependence was introduced.\nResults/limitations, if any.\nThis paper showed that poor quality-of-life scores are associated with drop out. Therefore, LMMs should be avoided when analyzing this data in clinical trials. The LMMs in both the historical studies and in the simulations were overly optimistic in estimating the quality-of-life scores. Specifically, the LMM overestimates the slope governing the prediction trajectory in both treatment and control arms. The LMM is also more optimistic for the control arm than the experimental arm, despite the protective effect of treatment on the quality-of-life score."
  },
  {
    "objectID": "literature.html#introduction-to-linear-mixed-models-oarc.ucla.edua",
    "href": "literature.html#introduction-to-linear-mixed-models-oarc.ucla.edua",
    "title": "Literature",
    "section": "INTRODUCTION TO LINEAR MIXED MODELS (oarc.ucla.edua)",
    "text": "INTRODUCTION TO LINEAR MIXED MODELS (oarc.ucla.edua)\nWhat is the goal of the paper?\nThe goal of the paper is to show how linear mixed models can help analyze data sets with non independence. Non independence can happen when data is hierarchical in structure resulting in correlations within groups in the data sets resulting in the violation of the assumption of independence between observations. The example provided has a sample dataset with different patient observations belonging to multiple doctors.\nWhy is it important?\nGoing on the aforementioned example, data can be averaged on a doctor group level but his would result in less data. And you can not make inferences about an individual patient. You can also do multiple linear models for each doctor but the models themselve have less data resulting in more noise.\nHow is it solved? – methods\nLinear Mixed models find a trade off between both approaches where the random effects are still considered for each doctor but there is still an overall grand or fixed effect.\nResults/limitations, if any.\nThere was no dataset or experiment analyzed in this paper. It is just a technical overview."
  },
  {
    "objectID": "literature.html#linear-mixed-model-for-analyzing-longitudinal-data-a-simulation-study-of-children-growth-differences-sciencedirect.com",
    "href": "literature.html#linear-mixed-model-for-analyzing-longitudinal-data-a-simulation-study-of-children-growth-differences-sciencedirect.com",
    "title": "Literature",
    "section": "Linear Mixed Model for Analyzing Longitudinal Data: A Simulation Study of Children Growth Differences (sciencedirect.com)",
    "text": "Linear Mixed Model for Analyzing Longitudinal Data: A Simulation Study of Children Growth Differences (sciencedirect.com)\nWhat is the goal of the paper?\nThe goal of the paper is to use Linear Mixed Models to analyze multilevel data of developmental growth rates in children. Different covariance structures were modeled within the LMM to capture correlated data through time.\nWhy is it important?\nGrowth curves in children are usually represented as a 2 level data structure. At level 2 is the individual child and at the second level is each individual observation. Traditional linear models are not effective due to the non-independence of the data.\nHow is it solved? – methods\nThe simulation study found that the UN covariance performed the best although it suffered from efficiency because of the high number of parameters leaving the ARH(1) as a solid alternative. The data was also not naturally collected but simulated\nResults/limitations, if any.\nThe simulation study found that the UN covariance performed the best although it suffered from efficiency because of the high number of parameters leaving the ARH(1) as a solid alternative. The data was also not naturally collected but simulated"
  },
  {
    "objectID": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology-nih.gov",
    "href": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology-nih.gov",
    "title": "Literature",
    "section": "A brief introduction to mixed effects modelling and multi-model inference in ecology (nih.gov)",
    "text": "A brief introduction to mixed effects modelling and multi-model inference in ecology (nih.gov)\nWhat is the goal of the paper?\nThe goal of this paper is to serve as a guide on the end to end to end analysis from formulating a hypothesis to inference to explaining the model parameters as it regards to biological and ecological data. Advantages and disadvantages are discussed with the different options available at different parts of the linear mixed model analysis pipeline\nWhy is it important?\nThis article is relevant to our group especially, to those unfamiliar with Linear Mix Models like myself. As it regards to biological science, this paper helps the reader avoid common pitfalls when implementing LMMs\nHow is it solved? – methods\nThe paper has a play by play on the different stages of LMM analysis with tangible example that include data, code and several graphs.\nResults/limitations, if any\nThere is no study or actual experiment being analyzed by linear mixed models. Although it does have references to plenty of academic research papers"
  },
  {
    "objectID": "literature.html#level-logical-explanations-visualizations-of-estimates-in-linear-mixed-models-recommendations-for-reporting-multilevel-data-and-analyses",
    "href": "literature.html#level-logical-explanations-visualizations-of-estimates-in-linear-mixed-models-recommendations-for-reporting-multilevel-data-and-analyses",
    "title": "Literature",
    "section": "LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models): recommendations for reporting multilevel data and analyses",
    "text": "LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models): recommendations for reporting multilevel data and analyses\nWhat is the goal of the paper?\nResearchers use LLMs to study hierarchical data and often report them under different names like mixed effects models, multilevel data, contextual analysis and hierarchical studies. There is no standardization across these papers for analyzing hierarchical data which leads to different aspects being reported. The goal of the paper is to make a standardized process for analyzing multilevel data\nWhy is it important?\nThis is important so studies across different time periods can be compared more easily.\nHow is it solved? – methods\nThe paper suggests using the LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models) as framework for conducting studies with reporting recommendations\nResults/limitations, if any.\nLack of flexibility. Sticking to a framework can inhibit creative analysis since you’re always looking at the framework for guidance."
  },
  {
    "objectID": "literature.html#estimation-and-selection-in-linear-mixed-models-with-missing-data-under-compound-symmetric-structure-nih.gov",
    "href": "literature.html#estimation-and-selection-in-linear-mixed-models-with-missing-data-under-compound-symmetric-structure-nih.gov",
    "title": "Literature",
    "section": "Estimation and selection in linear mixed models with missing data under compound symmetric structure (nih.gov)",
    "text": "Estimation and selection in linear mixed models with missing data under compound symmetric structure (nih.gov)\nWhat is the goal of the paper?\nMissing values occur all the time in real data. Statisticians and scientists use linear mixed models as a way to circumvent this issue. This paper aims to examine the estimation and model selection performance when faced with different rates of missing data. The paper employs two types of missing data. Missing at random and not at random.\nWhy is it important?\nGiven the frequency of missing data it’s important to know the impact it has on the model’s results. It’s also important to be able to\nHow is it solved? – methods\nMissingness of data is recorded using an indicator based matrix and then a likelihood based estimator is made to capture the probability of distribution of the observed data given the model parameters.\nResults/limitations, if any.\nThere is adequate model performance when there is a moderate amount of missingness in the data. However, the paper focuses on compound symmetric structures which assumes equal variance among any given pair of observations."
  },
  {
    "objectID": "literature.html#when-to-use-mixed-models",
    "href": "literature.html#when-to-use-mixed-models",
    "title": "Literature",
    "section": "When to use mixed models",
    "text": "When to use mixed models\nWhat is the goal of the paper?\nThe goal of the paper is to provide insights into the appropriate usage of mixed models in data science projects. It aims to discuss the types of outcome variables that mixed models can handle, highlight their advantages and disadvantages, and offer guidance on when to use them effectively.\nWhy is it important?\nUnderstanding when to employ mixed models is crucial for data scientists as these models offer unique capabilities, such as handling nested data structures and accommodating multiple readings on the same subject. Utilizing mixed models appropriately can improve the accuracy and interpretability of statistical analyses, especially in scenarios involving complex data structures.\nHow is it solved? – methods\nThe paper discusses the advantages and disadvantages of mixed models, outlining scenarios where they are beneficial and situations where they may not be necessary. It provides examples to illustrate the application of mixed models in scenarios involving hierarchical data structures and multiple measurements on the same subject. Additionally, the paper offers guidance on model selection and references related articles for further exploration.\nResults/limitations, if any.\nThe paper presents several advantages of mixed models, including their ability to handle nested data, provide interpretable coefficients, and accommodate missing measurements."
  },
  {
    "objectID": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r",
    "href": "literature.html#an-introduction-to-linear-mixed-effects-modeling-in-r",
    "title": "Literature",
    "section": "An Introduction to Linear Mixed-Effects Modeling in R",
    "text": "An Introduction to Linear Mixed-Effects Modeling in R\nWhat is the goal of the paper?\nThe goal of the tutorial is to provide both theoretical understanding and practical guidance on implementing mixed-effects models in R, particularly for researchers with basic statistical knowledge but limited experience in using these models. It aims to address the limitations of traditional statistical methods like repeated measures ANOVAs in analyzing correlated data and to introduce mixed-effects modeling as a more flexible and appropriate approach.\nWhy is it important?\nUnderstanding mixed-effects modeling is crucial for researchers, especially in fields like experimental psychology where traditional methods may not adequately address the complexities of correlated data. By offering accessible explanations and practical examples, the tutorial aims to empower researchers to effectively analyze their data using mixed-effects models, thereby improving the quality and validity of their research findings.\nHow is it solved? – methods\nThe tutorial provides a theoretical introduction to mixed-effects modeling, explaining concepts such as fixed and random effects in simple terms. It contrasts mixed-effects modeling with traditional methods like repeated measures ANOVAs, highlighting the advantages of the former in handling correlated data and various types of response variables. Practical guidance is offered through R code snippets and example data, allowing readers to follow along and implement mixed-effects models in their own research.\nResults/limitations, if any.\nThe tutorial does not present empirical results but rather serves as an educational resource. It effectively communicates the benefits of mixed-effects modeling and provides step-by-step instructions for implementation in R."
  },
  {
    "objectID": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology",
    "href": "literature.html#a-brief-introduction-to-mixed-effects-modelling-and-multi-model-inference-in-ecology",
    "title": "Literature",
    "section": "A brief introduction to mixed effects modelling and multi-model inference in ecology",
    "text": "A brief introduction to mixed effects modelling and multi-model inference in ecology\nWhat is the goal of the paper?\nThe paper aims to provide best practices for applying linear mixed effects models (LMMs) to biological data, particularly in ecology and evolutionary studies. It seeks to address the complexities of ecological data and offer guidance on model selection, interpretation, and common pitfalls encountered during modeling.\nWhy is it important?\nWith the increasing use of LMMs in biological data analysis, particularly in ecology, establishing best practices is critical for enhancing the robustness of conclusions drawn from ecological and evolutionary studies. Effective application of LMMs can improve the accuracy and reliability of research findings.\nHow is it solved? – methods The paper discusses various aspects of applying LMMs to biological data, including model selection, error structure, data transformation, and methods for model selection. It emphasizes the importance of careful consideration and consultation with a statistician, particularly in complex situations.\nResults/limitations, if any\nWhile the paper does not present empirical results, it offers practical solutions and recommendations for researchers working with ecological data. It effectively communicates the advantages and disadvantages of LMMs and provides valuable insights for their application in ecology and evolutionary studies."
  },
  {
    "objectID": "literature.html#introduction-to-linear-mixed-models",
    "href": "literature.html#introduction-to-linear-mixed-models",
    "title": "Literature",
    "section": "INTRODUCTION TO LINEAR MIXED MODELS",
    "text": "INTRODUCTION TO LINEAR MIXED MODELS\nWhat is the goal of the paper?\nThe article aims to provide a step-by-step code implementation guide for Linear Mixed Models (LMMs) and to explain the introduction of random effects in these models.\nWhy is it important?\nUnderstanding random effects in mixed models is crucial for researchers as it allows for the consideration of variability within and between groups in hierarchical data structures. Properly defining and incorporating random effects ensures accurate modeling and interpretation, helping to avoid issues such as pseudoreplication and ensuring the independence of observations.\nHow is it solved? – methods\nThe article explains the concepts of crossed and nested random effects in mixed models. Crossed random effects occur when factors are not hierarchically structured and can be observed across multiple levels, while nested random effects occur when one factor is nested within another, forming a hierarchical structure. The importance of properly coding the data to explicitly define nested structures is emphasized.\nResults/limitations, if any.\nThe article does not present empirical results but serves as an educational resource on implementing LMMs with random effects."
  },
  {
    "objectID": "literature.html#robustness-of-linear-mixed-effects-models-to-violations-of-distributional-assumptions",
    "href": "literature.html#robustness-of-linear-mixed-effects-models-to-violations-of-distributional-assumptions",
    "title": "Literature",
    "section": "Robustness of linear mixed-effects models to violations of distributional assumptions",
    "text": "Robustness of linear mixed-effects models to violations of distributional assumptions\nWhat is the goal of the paper?\nThe paper aims to investigate the robustness of linear mixed-effects models (LMMs) in analyzing complex datasets commonly found in ecology and evolution. It evaluates the impact of various violations of distributional assumptions and missing random effect components on model estimates.\nWhy is it important?\nUnderstanding the robustness of LMMs is crucial for researchers working with complex ecological and evolutionary datasets. Despite potential violations of assumptions and missing components, LMMs are widely used in these fields. Assessing their robustness helps ensure the accuracy and reliability of model estimates, even in challenging scenarios.\nHow is it solved? – methods\nThe study evaluates the impact of skewed, bimodal, and heteroscedastic random effect and residual variances, as well as the effects of missing random effect terms and correlated fixed effect predictors on model estimates. It likely employs simulations or analytical approaches to systematically assess the performance of LMMs under various conditions.\nResults/limitations, if any.\nThe results indicate that while violations of assumptions may lead to slight biases and decreased precision in estimates, the overall robustness of LMMs allows for accurate and unbiased estimation of fixed and random effects."
  },
  {
    "objectID": "literature.html#model-selection-in-linear-mixed-effect-models",
    "href": "literature.html#model-selection-in-linear-mixed-effect-models",
    "title": "Literature",
    "section": "Model selection in linear mixed effect models",
    "text": "Model selection in linear mixed effect models\nWhat is the goal of the paper?\nThe goal of the paper is to improve variable selection and parameter estimation in linear mixed effect models, which are critical for analyzing longitudinal, panel, and cross-sectional data in various scientific domains.\nWhy is it important?\nImproving variable selection and parameter estimation is crucial because it directly impacts the accuracy and reliability of data analysis across scientific fields. Efficiently identifying relevant variables and accurately estimating their effects are essential for drawing valid conclusions from complex data structures.\nHow is it solved? – Methods\nThe authors introduce a simple, iterative procedure that employs the smoothly clipped absolute deviation (SCAD) penalty function to estimate and select both fixed and random effects in these models. This approach is highlighted for being a consistent variable selection method with some oracle properties, suggesting it can perform almost as well as if the true underlying model were known.\nResults/limitations, if any.\nThe approach’s effectiveness and efficiency are validated through simulation studies and real data analysis. Nevertheless, the paper also points out limitations, including the method’s dependence on certain conditions for its asymptotic properties to hold and the potential computational challenges encountered with high-dimensional datasets."
  },
  {
    "objectID": "literature.html#random-effects-structure-for-testing-interactions-in-linear-mixed-effects-models",
    "href": "literature.html#random-effects-structure-for-testing-interactions-in-linear-mixed-effects-models",
    "title": "Literature",
    "section": "Random Effects Structure for Testing Interactions in Linear Mixed-Effects Models",
    "text": "Random Effects Structure for Testing Interactions in Linear Mixed-Effects Models\nWhat is the goal of the paper?\nThe goal is to provide a more accurate method for testing interactions within linear mixed-effects models, critiquing existing guidelines and proposing new ones that emphasize the inclusion of random slopes for the highest-order combination of within-unit factors in interactions.\nWhy is it important?\nThis is important because accurately testing interactions in mixed-effects models is crucial for statistical analyses, especially in avoiding high Type I error rates. The paper aims to refine the approach to these models to ensure more reliable results.\nHow is it solved? – Methods\nThe author employs Monte Carlo simulations to test the proposed guidelines, demonstrating that neglecting critical random slopes can significantly increase the chance of a false rejection of the null hypothesis. Including appropriate random slopes in the model is shown to ensure better performance.\nResults/limitations, if any.\nThe findings highlight that including the correct random slopes in mixed-effects models greatly improves model performance, particularly in accurately testing interactions between categorical variables. However, the paper’s limitations include its focus on interactions between categorical variables and the specific conditions of the simulations used."
  },
  {
    "objectID": "literature.html#pymer4-connecting-r-and-python-for-linear-mixed-modeling",
    "href": "literature.html#pymer4-connecting-r-and-python-for-linear-mixed-modeling",
    "title": "Literature",
    "section": "Pymer4: Connecting R and Python for Linear Mixed Modeling",
    "text": "Pymer4: Connecting R and Python for Linear Mixed Modeling\nWhat is the goal of the paper?\nThe goal is to develop Pymer4, a tool that bridges R and Python for linear mixed modeling, addressing the gap in Python for a package as flexible as R’s lme4 for complex data analysis.\nWhy is it important?\nPymer4 is significant for providing Python users with an accessible, integrated tool for linear mixed modeling, which was previously lacking, enhancing the analytical capabilities within the Python ecosystem.\nHow is it solved? – Methods\nPymer4 offers a solution by by leveraging the rpy2 library, as it connects to R’s lme4 package, offering a Pythonic interface for mixed modeling that integrates well with scientific Python tools, simplifying the analysis process.\nResults/limitations, if any.\nPymer4 successfully extends lme4’s functionality to Python users, offering features like significance testing and data visualization integration, enhancing multilevel model analysis. The paper focuses on the tool’s capabilities without detailing specific limitations."
  },
  {
    "objectID": "literature.html#a-powerful-and-flexible-linear-mixed-model-framework-for-the-analysis-of-relative-quantification-rt-pcr-data",
    "href": "literature.html#a-powerful-and-flexible-linear-mixed-model-framework-for-the-analysis-of-relative-quantification-rt-pcr-data",
    "title": "Literature",
    "section": "A powerful and flexible linear mixed model framework for the analysis of relative quantification RT-PCR data",
    "text": "A powerful and flexible linear mixed model framework for the analysis of relative quantification RT-PCR data\nWhat is the goal of the paper?\nThe paper introduces a novel linear mixed model framework for analyzing relative quantification RT-PCR data, aiming to overcome the limitations of existing statistical methods by providing more accurate and flexible analysis tools.\nWhy is it important?\nThis framework is crucial for its potential to enhance the statistical power and flexibility in analyzing RT-PCR data, enabling researchers to conduct more reliable and varied analyses of gene expression across different experimental conditions.\nHow is it solved? – Methods\nThe method involves a sophisticated statistical approach that incorporates both fixed and random effects in a linear mixed model, allowing for a more nuanced analysis of RT-PCR data that accounts for various sources of variability.\nResults/limitations, if any.\nThe framework has been shown to yield more accurate and statistically powerful results compared to traditional methods, facilitating better decision-making in biological research. The paper thoroughly evaluates the model’s performance and discusses its applicability to a wide range of experimental designs."
  },
  {
    "objectID": "report.html#implementation-methods",
    "href": "report.html#implementation-methods",
    "title": "Report",
    "section": "",
    "text": "The implementation of LMMs has been facilitated by various software packages and programming languages. Brown (Brown 2021) provides a comprehensive guide to implementing LMMs in R, a widely used statistical programming language, offering a step-by-step walkthrough of model syntax without delving deeply into complex mathematical foundations. Additionally, Pymer4, developed by Jolly (Jolly 2018), bridges R and Python, offering Python users a flexible and integrated tool for linear mixed modeling by leveraging the capabilities of R’s lme4 package. This tool enhances the analytical capabilities within the Python ecosystem, making advanced statistical methods more accessible to a broader audience."
  },
  {
    "objectID": "report.html#industry-specific-uses",
    "href": "report.html#industry-specific-uses",
    "title": "Report",
    "section": "",
    "text": "LMMs find applications across various scientific domains, each with its unique data structures and analytical challenges. Magezi (Magezi 2015) highlights the use of LMMs in within-participant psychology experiments, addressing the complexities of repeated measures and nested data structures common in psychological research. Harrison Et. A. (Harrison et al. 2018) and Bolker Et. Al. (Bolker et al. 2009) discuss the application of LMMs and generalized linear mixed models (GLMMs) in ecology, emphasizing their utility in analyzing ecological data that involve complex relationships and hierarchical data structures. In the medical field, LMMs are employed to model pandemic-induced mortality changes, as demonstrated by Verbeeck Et. Al. (Verbeeck et al. 2023), and to analyze longitudinal health-related quality of life data in cancer clinical trials, as discussed by Touraine Et. Al. [Touraine et al. (2023)]."
  },
  {
    "objectID": "report.html#strengths-and-weaknesses",
    "href": "report.html#strengths-and-weaknesses",
    "title": "Report",
    "section": "",
    "text": "The strengths of LMMs lie in their flexibility to model complex data structures and their ability to handle missing data, making them a powerful tool for a wide range of scientific inquiries. However, their application is not without challenges. Peng and Lu (Peng and Lu 2012) address the difficulty of variable selection and parameter estimation in LMMs, proposing an iterative procedure to improve model accuracy. Barr (Barr 2013) critiques existing guidelines for testing interactions within LMMs, proposing new guidelines to ensure more reliable results. Despite their robustness, as noted by Schielzeth Et. Al. [Schielzeth et al. (2020)], LMMs require careful evaluation of model assumptions and may present computational challenges, especially with high-dimensional datasets."
  },
  {
    "objectID": "report.html#purpose",
    "href": "report.html#purpose",
    "title": "Report",
    "section": "",
    "text": "This literature review collectively emphasize the versatility, robustness, and broad applicability of LMMs and GLMMs across various fields of research. Despite their advantages, the importance of careful model selection, acknowledgment of limitations, and the potential need for more complex models such as joint models in certain scenarios are also highlighted. As the use of LMMs continues to grow, the development of standardized processes and user-friendly tools will be crucial in ensuring the accurate and effective application of these models in research."
  },
  {
    "objectID": "literature.html#to-transform-or-not-to-transform-using-generalized-linear-mixed-models-to-analyse-reaction-time-data",
    "href": "literature.html#to-transform-or-not-to-transform-using-generalized-linear-mixed-models-to-analyse-reaction-time-data",
    "title": "Literature",
    "section": "To transform or not to transform: using generalized linear mixed models to analyse reaction time data",
    "text": "To transform or not to transform: using generalized linear mixed models to analyse reaction time data\nWhat is the goal of the paper?\nThe paper aims to challenge the common practice of transforming reaction time (RT) data to meet normality assumptions in statistical analyses within cognitive psychology research. It proposes generalized linear mixed-effect models (GLMMs) as a solution to accurately analyze RT data without the need for transformation, thus avoiding potential theoretical implications and misleading conclusions.\nWhy is it important?\nIt highlights the discrepancy between analyses of raw RT data and transformed RT data, as demonstrated by Balota et al. (2013), emphasizing the need for a more nuanced approach to analyzing RT data. By advocating for GLMMs, the paper aims to promote proper assessment of individual differences and enhance the testing of cognitive theories.\nHow is it solved? – Methods\nThe study discusses the theoretical decisions involved in specifying a GLMM and provides reanalysis of datasets from Balota et al. (2013) to illustrate the application of GLMMs in RT data analysis. It emphasizes the importance of analyzing changes in RT distribution at a finer level to capture more accurate measures of group performance and effectively test cognitive theories.\nResults/limitations, if any.\nThe paper suggests that GLMMs offer a more robust approach to analyzing RT data compared to traditional methods like linear mixed-effect models (LMMs) with transformed data. However, it acknowledges the complexities of addressing skewed dependent variables like RT in LMMs and the potential challenges in adopting GLMMs, such as the need for careful model specification."
  },
  {
    "objectID": "literature.html#analysing-disease-incidence-data-from-designed-experiments-by-generalized-linear-mixed-models",
    "href": "literature.html#analysing-disease-incidence-data-from-designed-experiments-by-generalized-linear-mixed-models",
    "title": "Literature",
    "section": "Analysing disease incidence data from designed experiments by generalized linear mixed models",
    "text": "Analysing disease incidence data from designed experiments by generalized linear mixed models\nWhat is the goal of the paper?\nThe paper aims to introduce generalized linear mixed models (GLMMs) as a robust method for analyzing disease incidence data from designed experiments, specifically addressing overdispersion issues common in epidemiological research.\nWhy is it important?\nIt highlights the inadequacy of traditional methods like ANOVA for such data, underlining the need for alternative approaches like GLMMs to better capture the complexities of disease clustering and aggregation.\nHow is it solved? – Methods\nThe study presents GLMMs as a versatile tool, capable of accommodating both fixed and random effects, thus offering a more flexible framework for analyzing disease incidence data. It illustrates the application of GLMMs using real-world data from an experiment on downy mildew incidence in grapevines.\nResults/limitations, if any.\nThe analysis using GLMMs reveals significant treatment effects and provides parameter estimates. However, the approach is not without limitations, including assumptions made in modeling and the potential challenge of interpreting results accurately, particularly in complex experimental designs."
  },
  {
    "objectID": "literature.html#fitting-linear-mixed-effects-models-using-lme4-journal-of-statistical-software-jstatsoft.org",
    "href": "literature.html#fitting-linear-mixed-effects-models-using-lme4-journal-of-statistical-software-jstatsoft.org",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe paper presents the lme4 package for R, which facilitates the fitting of linear mixed-effects models. The authors aim to articulate the package’s capabilities in evaluating the profiled deviance or REML criterion for linear mixed models and to explain the representation and optimization of such models for parameter estimation.\nWhy is it important?\nThe significance of this paper lies in its contribution to the field of computational methods for fitting mixed models—an area with many open problems. The lme4 package represents an evolution in this domain, offering more efficient computational tools and a syntax that simplifies the modeling process, especially for models with crossed random effects.\nHow is it solved? – methods\nThe package utilizes maximum likelihood or restricted maximum likelihood (REML) estimates for linear mixed-effects model parameters, employing numerical representation and optimization functions within R. The paper delves into the model’s structure, the evaluative steps for the profiled deviance or REML criterion, and the class structure representing such models, highlighting the improvements over previous formulations.\nResults/limitations, if any.\nThe document focuses more on methodology than specific results or limitations. It details the improvement over the nlme package, addressing efficient linear algebra tools and the incorporation of profile likelihood confidence intervals on random-effects parameters. The paper emphasizes the ongoing development of the lme4 package, acknowledging the need for stability and usability for a broad range of applications."
  },
  {
    "objectID": "literature.html#statistical-primer-an-introduction-to-the-application-of-linear-mixed-effects-models-in-cardiothoracic-surgery-outcomes-research-a-case-study-using-homograft-pulmonary-valve-replacement-data---pubmed-nih.gov",
    "href": "literature.html#statistical-primer-an-introduction-to-the-application-of-linear-mixed-effects-models-in-cardiothoracic-surgery-outcomes-research-a-case-study-using-homograft-pulmonary-valve-replacement-data---pubmed-nih.gov",
    "title": "Literature",
    "section": "",
    "text": "What is the goal of the paper?\nThe goal of the paper is to provide a detailed introduction to developing and interpreting linear mixed-effects models for repeated measurements in the context of cardiothoracic surgery outcomes research. The paper uses a dataset on patients undergoing surgical pulmonary valve replacement to illustrate the steps of developing such models for clinician researchers.\nWhy is it important?\nThis work is important because the emergence of large cardio-thoracic surgery datasets, including repeated measurements over time, presents an opportunity to apply advanced modeling of outcomes. Linear mixed-effects models offer a more nuanced understanding of these outcomes compared to traditional methods, which is crucial for enhancing clinical decision-making and patient care.\nHow is it solved? – methods\nThe authors used a retrospective dataset containing serial echocardiographic measurements from patients who underwent surgical pulmonary valve replacement at Erasmus MC between 1986 and 2017. The paper discusses the construction of the model, including dealing with missing values, correlated variables, and multicollinearity. It also covers model specification, variable selection, addressing nonlinearity, and interpretation of results. An R script is provided for implementing the model.\nResults/limitations, if any.\nThe paper illustrates the construction of the model, including essential aspects such as theories of linear mixed-effects models, missing values, collinearity, interaction, nonlinearity, model specification, and results interpretation. It shows that linear mixed-effects models provide a more detailed view of repeated measurements and give more valid estimates compared to linear regression models, especially in the context of cardio-thoracic surgery outcomes research. Limitations related to model assumptions, such as linearity and normal distribution of residuals, are addressed through transformations and statistical tests."
  },
  {
    "objectID": "report.html#mathematical-foundations",
    "href": "report.html#mathematical-foundations",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "2.1 Mathematical Foundations",
    "text": "2.1 Mathematical Foundations\nLMMs can be defined as:\n\\(y=X\\beta + Z\\gamma + \\epsilon\\)\nwhere:\n\nY is the response vector.\nX is the design matrix for fixed effects.\nβ is the vector of fixed effects (parameters associated with the entire population or certain repeatable levels of experimental factors).\nZ is the design matrix for random effects.\nγ is the vector of random effects (represent random deviations from the population parameters (β ) for different subjects or experimental units; i.e., the variability not explained by the fixed effects).\nϵ is the vector of residual errors.\n\nAlthough more flexible than other methods such as ANOVA, there are several assumptions for LMMs:\n\nRandom effects (γ) are assumed to follow a normal distribution with mean zero and variance-covariance matrix G.\n\\(\\gamma \\sim N(0,G)\\)\nResidual errors (ϵ ) are assumed to follow a normal distribution with mean zero and variance-covariance matrix R.\n\\(\\epsilon \\sim N(0,R)\\)\nRandom effects (γ) and residual errors (ϵ ) are assumed to be independent."
  },
  {
    "objectID": "report.html#breakdown-of-mathematical-foundations",
    "href": "report.html#breakdown-of-mathematical-foundations",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "2.2 Breakdown of Mathematical Foundations",
    "text": "2.2 Breakdown of Mathematical Foundations\n[PLACEHOLDER FOR LAYMANS TERMS EXPLANATIONS]"
  },
  {
    "objectID": "report.html#sample-data-structure",
    "href": "report.html#sample-data-structure",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "2.3 Sample Data Structure",
    "text": "2.3 Sample Data Structure\n[PLACEHOLDER]"
  },
  {
    "objectID": "report.html#packages",
    "href": "report.html#packages",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "3.1 Packages",
    "text": "3.1 Packages\n\nif (!requireNamespace(c(\"tidyverse\", \"lme4\", \"nlme\", \"gt\", \"RefManageR\"), quietly = TRUE)) {\n    install.packages(c(\"tidyverse\", \"lme4\", \"nlme\", \"gt\", \"RefManageR\"))\n}\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(nlme)\n\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nlibrary(gt)\nlibrary(RefManageR)\nreferences &lt;- ReadBib(\"references.bib\")\nsummary(references)\n\n                           Length Class    Mode\nbrown_introduction_2021    1      BibEntry list\nharrison_brief_2018        1      BibEntry list\nbolker_generalized_2009    1      BibEntry list\nmagezi_linear_2015         1      BibEntry list\nverbeeck_linear_2023       1      BibEntry list\ntouraine_when_2023         1      BibEntry list\npusponegoro_linear_2017    1      BibEntry list\nmonsalves_level_2020       1      BibEntry list\nlee_estimation_nodate      1      BibEntry list\nschielzeth_robustness_2020 1      BibEntry list\npeng_model_2012            1      BibEntry list\nbarr_random_2013           1      BibEntry list\nsteibel_powerful_2009      1      BibEntry list\njolly_pymer4_2018          1      BibEntry list\nbates_fitting_2015         1      BibEntry list\nwang_statistical_2022      1      BibEntry list\nlo_transform_2015          1      BibEntry list\npiepho_analysing_1999      1      BibEntry list\ntu_using_2015              1      BibEntry list\nfokkema_generalized_2021   1      BibEntry list\ngrueber_2011               1      BibEntry list\nzuur_2016                  1      BibEntry list\naarts_2015                 1      BibEntry list\ncasals_methodological_2014 1      BibEntry list\n\n\n\ntidyverse\nlme4\nnlme\ngt"
  },
  {
    "objectID": "report.html#data-ingestion",
    "href": "report.html#data-ingestion",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "3.2 Data Ingestion",
    "text": "3.2 Data Ingestion\n\n# Load the datasets\ndata(\"sleepstudy\", package = \"lme4\")  # From lme4\ndata(\"Orthodont\", package = \"nlme\")   # From nlme\ndata(\"Dyestuff\", package = \"lme4\")    # From lme4\nBMI &lt;- read.csv(\"data/BMI_IOS_SCD_Asthma.csv\")\n\nstr(sleepstudy)\n\n'data.frame':   180 obs. of  3 variables:\n $ Reaction: num  250 259 251 321 357 ...\n $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...\n $ Subject : Factor w/ 18 levels \"308\",\"309\",\"310\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nstr(Orthodont)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  108 obs. of  4 variables:\n $ distance: num  26 25 29 31 21.5 22.5 23 26.5 23 22.5 ...\n $ age     : num  8 10 12 14 8 10 12 14 8 10 ...\n $ Subject : Ord.factor w/ 27 levels \"M16\"&lt;\"M05\"&lt;\"M02\"&lt;..: 15 15 15 15 3 3 3 3 7 7 ...\n $ Sex     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"outer\")=Class 'formula'  language ~Sex\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n - attr(*, \"formula\")=Class 'formula'  language distance ~ age | Subject\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Age\"\n  ..$ y: chr \"Distance from pituitary to pterygomaxillary fissure\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(yr)\"\n  ..$ y: chr \"(mm)\"\n - attr(*, \"FUN\")=function (x)  \n  ..- attr(*, \"source\")= chr \"function (x) max(x, na.rm = TRUE)\"\n - attr(*, \"order.groups\")= logi TRUE\n\nstr(Dyestuff)\n\n'data.frame':   30 obs. of  2 variables:\n $ Batch: Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 2 2 2 2 2 ...\n $ Yield: num  1545 1440 1440 1520 1580 ...\n\nstr(BMI)\n\n'data.frame':   219 obs. of  16 variables:\n $ Group             : chr  \"C-SCD\" \"C-SCD\" \"C-SCD\" \"C-SCD\" ...\n $ Subject.ID        : int  1 1 1 1 2 3 3 4 4 5 ...\n $ Observation_number: int  1 2 3 4 1 1 2 1 2 1 ...\n $ Hydroxyurea       : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Asthma            : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ ICS               : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ LABA              : chr  \"No\" \"No\" \"Yes\" \"Yes\" ...\n $ Gender            : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ Age..months.      : int  239 193 212 224 204 178 186 222 210 196 ...\n $ Height..cm.       : num  164 163 164 164 154 ...\n $ Weight..Kg.       : num  61.5 62.3 63.1 63.7 66.4 51.9 56.7 66.7 66.9 52.9 ...\n $ BMI               : num  22.8 23.5 23.6 23.7 27.8 ...\n $ R5Hz_PP           : int  145 103 107 87 124 109 117 101 179 136 ...\n $ R20Hz_PP          : int  133 98 98 87 121 86 105 132 153 97 ...\n $ X5Hz_PP           : num  -456 111 174 -303 98 115 107 -216 195 140 ...\n $ Fres_PP           : int  NA 169 159 NA 135 148 159 NA 175 199 ...\n\n\n[PLACEHOLDER TO DESCRIBE THE DATASET/VARIABLES/GROUPS]\n\n1. sleepstudy from the lme4 package\n\nDescription: This dataset contains the reaction times of subjects over several days of sleep deprivation.\nWhy suitable for LMMs: The sleepstudy data are ideal for LMMs because they include repeated measures for each subject, leading to correlated observations within subjects. An LMM can model the fixed effect of days of sleep deprivation on reaction time while accounting for random subject-specific intercepts and possibly slopes, capturing individual variability in reaction time and the effect of sleep deprivation.\n\n\n\n2. Orthodont from the nlme package\n\nDescription: This dataset contains measurements of the distance from the pituitary to the pterygomaxillary fissure in 27 children, measured at ages 8, 10, 12, and 14.\nWhy suitable for LMMs: The repeated measurements for each child create a nested data structure (measurements within children), leading to non-independence of observations. An LMM can effectively model the growth curves by including child-specific random effects, allowing for individual variations in growth trajectories.\n\n\n\n3. Dyestuff from the lme4 package\n\nDescription: This dataset contains the yield from a dyeing process for different batches of dye.\nWhy suitable for LMMs: The dataset has a grouping factor (batches), and there may be batch-to-batch variability that affects the yield. An LMM can include random effects for batches to capture this variability, which simpler models like linear regression would not account for.\n\n\n\n4. BMI from Kaggle\n\nDescription: This dataset is from a retrospective study to assess the impact of BMI on impulse oscillometry (IOS) estimates of airway resistance and reactance in children with sickle cell disease (C-SCD)\nWhy suitable for LMMs: The dataset has multiple observations, over time, for the same set of participants."
  },
  {
    "objectID": "report.html#exploratory-data-analysis-eda",
    "href": "report.html#exploratory-data-analysis-eda",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "3.2 Exploratory Data Analysis (EDA)",
    "text": "3.2 Exploratory Data Analysis (EDA)\n[PLACEHOLDER FOR TABLES/VIS]"
  },
  {
    "objectID": "report.html#linear-mixed-modeling",
    "href": "report.html#linear-mixed-modeling",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "3.3 Linear Mixed Modeling",
    "text": "3.3 Linear Mixed Modeling"
  },
  {
    "objectID": "report.html#model-comparisons-and-assessment",
    "href": "report.html#model-comparisons-and-assessment",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "3.4 Model Comparisons and Assessment",
    "text": "3.4 Model Comparisons and Assessment"
  },
  {
    "objectID": "report.html#simulation-possibly",
    "href": "report.html#simulation-possibly",
    "title": "[PLACEHOLDER FOR BETTER TITLE]",
    "section": "3.5 Simulation (POSSIBLY)",
    "text": "3.5 Simulation (POSSIBLY)"
  },
  {
    "objectID": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov",
    "href": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov",
    "title": "Literature",
    "section": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)",
    "text": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)\nWhat is the goal of the paper?\nThe goal of the paper is to highlight obstacles when model averaging and using information theoretic framework and their potential solutions if they exist\nWhy is it important?\nA large number of ecologists and biologists are analyzing data with the Information theoretic framework rather than the traditional hypothesis testing. Modeling averaging becomes increasingly difficult with Linear Mixed models due to the fixed and random effects.\nHow is it solved? – methods\nThe research suggests that researchers define appropriate inputs and predictor variables and to handle collinearity with extreme care especially when dealing with the random effects of LMMs.The paper then goes on to propose strategies on model averaging and definition top model sets.\nResults/limitations, if any.\nAs the paper mentions, it is NOT an exhaustive survival of the potential problems of applying model averaging under an IT framework. Some problems still exist like determining which IT criteria to use when comparing models with random factors."
  },
  {
    "objectID": "literature.html#a-protocol-for-conducting-and-presenting-results-of-regression-type-analyses",
    "href": "literature.html#a-protocol-for-conducting-and-presenting-results-of-regression-type-analyses",
    "title": "Literature",
    "section": "A protocol for conducting and presenting results of regression-type analyses",
    "text": "A protocol for conducting and presenting results of regression-type analyses\nWhat is the goal of the paper?\nThe goal of the paper is to streamline analysis by giving the reader a 10 step protocol. It helps fellow researchers select models, justify assumptions, and validate models.\nWhy is it important?\nThis paper is great for researchers new to Linear Mixed models and are looking to use its advantages on a dataset. The protocol is extremely straightforward. Linear mixed models offer more in depth analysis and it is important that all researchers know it to further the field of ecology.\nHow is it solved? – methods\nThe paper has 10 steps with very concrete examples that include sample datasets, visualizations, and results. The reader has something tangible and can directly apply what they learned to another dataset.\nResults/limitations, if any\nThe paper is limited by showing sample results and shows no empirical data. It identifies potential pitfalls like overdispersion of fitted models but offers no real solution"
  },
  {
    "objectID": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov-1",
    "href": "literature.html#multimodel-inference-in-ecology-and-evolution-challenges-and-solutionsnih.gov-1",
    "title": "Literature",
    "section": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)",
    "text": "Multimodel inference in ecology and evolution: challenges and solutions(nih.gov)\nWhat is the goal of the paper?\nThe goal of the paper is to highlight obstacles when model averaging and using information theoretic framework and their potential solutions if they exist\nWhy is it important?\nA large number of ecologists and biologists are analyzing data with the Information theoretic framework rather than the traditional hypothesis testing. Modeling averaging becomes increasingly difficult with Linear Mixed models due to the fixed and random effects.\nHow is it solved? – methods\nThe research suggests that researchers define appropriate inputs and predictor variables and to handle collinearity with extreme care especially when dealing with the random effects of LMMs.The paper then goes on to propose strategies on model averaging and definition top model sets.\nResults/limitations, if any.\nAs the paper mentions, it is NOT an exhaustive survival of the potential problems of applying model averaging under an IT framework. Some problems still exist like determining which IT criteria to use when comparing models with random factors."
  },
  {
    "objectID": "literature.html#using-generalized-linear-mixed-models-to-evaluate-inconsistency-within-a-network-meta-analysis",
    "href": "literature.html#using-generalized-linear-mixed-models-to-evaluate-inconsistency-within-a-network-meta-analysis",
    "title": "Literature",
    "section": "Using Generalized Linear Mixed Models to Evaluate Inconsistency within a Network Meta-Analysis",
    "text": "Using Generalized Linear Mixed Models to Evaluate Inconsistency within a Network Meta-Analysis\nWhat is the goal of the paper?\nThe goal of the paper is to demonstrate how generalized linear mixed models (GLMMs) can evaluate inconsistency within network meta-analyses, improving upon traditional models by using an arm-based approach for more accurate results.\nWhy is it important?\nThe paper addresses the challenge of inconsistency between direct and indirect evidence in network meta-analysis, which can compromise the validity of conclusions, offering a more reliable framework for analysis.\nHow is it solved? – Methods\nThe authors propose an arm-based GLMM approach, which allows for flexible modeling of different outcome variables and shows improved accuracy over contrast-based methods, especially when event rates are low.\nResults/limitations, if any.\nThe arm-based model provided more accurate evaluations of design inconsistency and treatment effects compared to traditional contrast-based approaches, highlighting its utility in complex analyses involving many treatments and designs."
  },
  {
    "objectID": "literature.html#generalized-linear-mixed-model-glmm-trees-a-flexible-decision-tree-method-for-multilevel-and-longitudinal-data",
    "href": "literature.html#generalized-linear-mixed-model-glmm-trees-a-flexible-decision-tree-method-for-multilevel-and-longitudinal-data",
    "title": "Literature",
    "section": "Generalized linear mixed-model (GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data",
    "text": "Generalized linear mixed-model (GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data\nWhat is the goal of the paper?\nThe goal of the paper is to introduce GLMM trees, a method combining generalized linear mixed models (GLMMs) with decision trees for analyzing multilevel and longitudinal data, offering a novel approach to clinical prediction problems.\nWhy is it important?\nGLMM trees provide an interpretable and flexible method that can handle complex data structures, improving upon traditional models by simplifying the analysis and enhancing the understanding of data patterns.\nHow is it solved? – Methods\nThe paper employs GLMM trees to analyze a large dataset from UK mental health services, comparing its performance with traditional GLMMs and random forests to demonstrate its predictive accuracy and efficiency.\nResults/limitations, if any.\nThe method achieves similar predictive accuracy to traditional GLMMs and random forests but with fewer variables, showcasing its potential to streamline clinical decision-making."
  },
  {
    "objectID": "literature.html#multilevel-analysis-quantifies-variation-in-the-experimental-effect-while-optimizing-power-and-preventing-false-positives",
    "href": "literature.html#multilevel-analysis-quantifies-variation-in-the-experimental-effect-while-optimizing-power-and-preventing-false-positives",
    "title": "Literature",
    "section": "Multilevel analysis quantifies variation in the experimental effect while optimizing power and preventing false positives",
    "text": "Multilevel analysis quantifies variation in the experimental effect while optimizing power and preventing false positives\nWhat is the goal of the paper?\nThe goal of this is to show how to handle nested data in neuroscience. Oftentimes data will be collected from the same sample with different experimental conditions. The paper states that this often goes over look in neuroscience\nWhy is it important?\nNot only are several assumptions violated but experiential effects could be miscalculated leading to representing incorrect results.\nHow is it solved? – methods\nThe paper does two simulation studies to show the significance of using the appropriate significant method. Design A had cluster data that may have random effects for just the intercept.. Design B had cluster data that may have random effects in both the intercepts and experiment effect. Both cases resulted in an increase in false positive rates.\nResults/limitations, if any.\nThis data is simulated and only within the context of neuroscience data. It would be beneficial to see these results to a real world dataset"
  }
]